# -*- coding: utf-8 -*-
"""CV_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KIiR92QgThHL-_tooVs1rQhwGIewtEW7
"""

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torchsummary import summary
from torch.utils.data import random_split, DataLoader, Dataset
import torchvision.transforms as transforms
import numpy as np
import torch.optim as optim
import torch.utils as utils
import matplotlib.pyplot as plt

def Visualization(results):
  values = list(results.values())
  labels = list(results.keys())
  n = len(values[0])
  plt.figure(figsize=(15, 10))
  for i in range(len(labels)):
    plt.subplot(2, len(labels)//2, i + 1)
    plt.plot(np.linspace(0,n,n), values[i])
    plt.title(labels[i])
  plt.show()

def show(img):
  img = img.detach().numpy()
  img = np.transpose(img, (1,2,0))
  plt.imshow(img)


def display(b_images, cl_images, pred_image=None):
  labels_clear = ['Original Image']*len(cl_images)
  labels_blur = ['Blur Image']*len(b_images)
  labels_pred = ['Predicted Image']*len(b_images)

  if pred_image is not None:
    labels = [labels_clear[0], labels_blur[0], labels_pred[0]]
    img = [cl_images[0], b_images[0], pred_image[0]]
    plt.figure(figsize=(15,15))

  else:
    labels = [labels_clear[0], labels_blur[0]]
    img = [cl_images[0], b_images[0]]
    plt.figure(figsize=(10,10))


  
  n = len(labels)
  for i in range(n):
    plt.subplot(1,n, i+1)
    plt.title(labels[i])
    show(img[i]*0.5+0.5)
    plt.axis('off')
  plt.show()

class DenseLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(DenseLayer, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=3 // 2)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        return torch.cat([x, self.relu(self.conv(x))], 1)

class RDB(nn.Module):
    def __init__(self, in_channels, growth_rate, num_layers):
        super(RDB, self).__init__()
        self.layers = nn.Sequential(*[DenseLayer(in_channels + growth_rate * i, growth_rate)\
         for i in range(num_layers)])

        # local feature fusion
        # self.lff = nn.Conv2d(in_channels + growth_rate * num_layers, growth_rate, kernel_size=1)
        
        self.lff = nn.Conv2d(in_channels + growth_rate * num_layers, growth_rate, kernel_size=1)
        

    def forward(self, x):
        return x + self.lff(self.layers(x))  # local residual learning

class RDN_model(nn.Module):
    def __init__(self, num_channels, num_features, growth_rate, num_blocks, num_layers):
        super(RDN_model, self).__init__()
        self.G0 = num_features
        self.G = growth_rate
        self.D = num_blocks
        self.C = num_layers

        # shallow feature extraction
        self.sfe1 = nn.Conv2d(num_channels, num_features, kernel_size=3, padding=3 // 2)
        self.sfe2 = nn.Conv2d(num_features, num_features, kernel_size=3, padding=3 // 2)

        # residual dense blocks
        self.rdbs = nn.ModuleList([RDB(self.G0, self.G, self.C)])
        for _ in range(self.D - 1):
            self.rdbs.append(RDB(self.G, self.G, self.C))
        else:
          self.rdbs = nn.ModuleList([RDB(self.G0, self.G, self.C)])
          for _ in range(self.D - 1):
            self.rdbs.append(RDB(self.G, self.G, self.C))
          


        # global feature fusion
        self.gff = nn.Sequential(
            nn.Conv2d(self.G * self.D, self.G0, kernel_size=1),
            nn.Conv2d(self.G0, self.G0, kernel_size=3, padding=3 // 2)
        )

        # 1x1 cov
        
        self.conv1x1 = nn.Conv2d(self.G0, num_channels, kernel_size=1)
        

    def forward(self, x):
        inp = x
        sfe1 = self.sfe1(x)
        sfe2 = self.sfe2(sfe1)

        x = sfe2
        local_features = []
        for i in range(self.D):
            x = self.rdbs[i](x)
            local_features.append(x)

        x = self.gff(torch.cat(local_features, 1)) + sfe1  # global residual learning
        x = self.conv1x1(x)
        x = x + inp

        return x

class RDN(nn.Module):
    def __init__(self, scale_factor, num_channels, num_features, growth_rate, num_blocks, num_layers):
        super(RDN, self).__init__()
        self.G0 = num_features
        self.G = growth_rate
        self.D = num_blocks
        self.C = num_layers

        # shallow feature extraction
        self.sfe1 = nn.Conv2d(num_channels, num_features, kernel_size=3, padding=3 // 2)
        self.sfe2 = nn.Conv2d(num_features, num_features, kernel_size=3, padding=3 // 2)

        # residual dense blocks
        self.rdbs = nn.ModuleList([RDB(self.G0, self.G, self.C)])
        for _ in range(self.D - 1):
            self.rdbs.append(RDB(self.G, self.G, self.C))

        # global feature fusion
        self.gff = nn.Sequential(
            nn.Conv2d(self.G * self.D, self.G0, kernel_size=1),
            nn.Conv2d(self.G0, self.G0, kernel_size=3, padding=3 // 2)
        )

        # up-sampling
        assert 1 <= scale_factor <= 4
        if scale_factor == 2 or scale_factor == 4 or scale_factor == 1:
            self.upscale = []
            for _ in range(scale_factor // 2):
                self.upscale.extend([nn.Conv2d(self.G0, self.G0 * (2 ** 2), kernel_size=3, padding=3 // 2),
                                     nn.PixelShuffle(2)])
            self.upscale = nn.Sequential(*self.upscale)
        else:
            self.upscale = nn.Sequential(
                nn.Conv2d(self.G0, self.G0 * (scale_factor ** 2), kernel_size=3, padding=3 // 2),
                nn.PixelShuffle(scale_factor)
            )

        self.output = nn.Conv2d(self.G0, num_channels, kernel_size=3, padding=3 // 2)
        #self.out = nn.Conv2d(num_channels, num_channels, 1, padding=0)

    def forward(self, x):
        sfe1 = self.sfe1(x)
        sfe2 = self.sfe2(sfe1)

        x = sfe2
        local_features = []
        for i in range(self.D):
            x = self.rdbs[i](x)
            local_features.append(x)

        x = self.gff(torch.cat(local_features, 1)) + sfe1  # global residual learning
        x = self.upscale(x)
        x = self.output(x)
        #x = self.out(x)
        return x

def Accuracy(real, pred):
  # MAE: Mean Absolute Error
  abs_diff = torch.abs(torch.subtract(real, pred))
  return torch.mean(abs_diff)

def Mean(L):
    from functools import reduce
    if not L:
        return 0
    else:
        return reduce(lambda x, y: x + y, L) / len(L)

import functools
def PSNR(img_a, img_b, max=1.0):
  assert img_a.shape == img_b.shape
  mse = torch.mean(torch.square(torch.subtract(img_a, img_b)))
  return 20*torch.log10(max/mse)

def train_step(model:nn.Module, 
               dataloader, epochs, val_dataloader=None, num_step=15000, val_num_step=5000, patience=2):
  criterion = nn.L1Loss()
  optimizer = optim.Adam(model.parameters(), lr=0.0001)
  early_stopping = False
  store_ = []
  train_loss = []
  train_ssim = []
  train_psnr = []
  val_loss = []
  val_ssim = []
  val_psnr = []
  results = {}
  
  
  for epoch in range(epochs):
    train_loss_ = []
    train_ssim_ = []
    psnr_ = []

    start = time.time()

    for i, data in enumerate(dataloader):
      inputs, labels = data[0].to(device), data[1].to(device)

      

      # Zeros the parameter gradients
      optimizer.zero_grad()

      # forward + backward + optimize
      outputs = model(inputs)
      loss = criterion(outputs, labels)
      loss.backward()
      optimizer.step()

      # print statistics
      
      train_loss_.append(loss.item())
      train_ssim_.append(ssim(outputs, labels, 1.0).item())
      psnr_.append(PSNR(labels, outputs).item())
      
      if i == num_step:
        
        break
    end = time.time()

    if val_dataloader is not None:
      
      with torch.no_grad():
       
        val_loss_ = []
        val_ssim_ = []
        val_psnr_ = []
        
  
        for j, dt in enumerate(val_dataloader):
          val_inp, val_label = dt[0].to(device), dt[1].to(device)
          val_pred_inp = model(val_inp)
          val_loss_.append(criterion(val_label, val_pred_inp).item())
          val_ssim_.append(ssim(val_label, val_pred_inp, 1.0).item())
          val_psnr_.append(PSNR(val_label, val_pred_inp).item())
          if j == val_num_step:
            break
          end = time.time()

        # Implement Early stopping        
        store_.append(Mean(val_loss_))
        if len(store_) == patience + 1:
          current = store_[-1]
          previous = store_[:len(store_)]
          
          if all([np.allclose(current, p) for p in previous]):
            #early_stopping = True
            store_ = []
          else:
            store_ = []
        
        
        #display(val_inp.cpu().data, val_label.cpu().data, val_pred_inp.cpu().data)
        print(f"Epoch: {epoch} Time: {(end - start):.5f} train loss:{Mean(train_loss_):0.3f} train ssim: {Mean(train_ssim_):.3f} train psnr: {Mean(psnr_):.3f} val loss: {Mean(val_loss_):.3f} val ssim: {Mean(val_ssim_):.3f} val psnr: {Mean(val_psnr_):.3f}")

        train_loss.append(Mean(train_loss_))
        train_ssim.append(Mean(train_ssim_))
        train_psnr.append(Mean(psnr_))
        val_loss.append(Mean(val_loss_))
        val_ssim.append(Mean(val_ssim_))
        val_psnr.append(Mean(val_psnr_))
        
        
    else:
      print(f"Epoch: {epoch} Time: {(end - start):.5f} Train loss: {Mean(train_loss_):0.3f} Train ssim: {Mean(train_ssim_):.3f}")

      train_loss.append(Mean(train_loss_))
      train_ssim.append(Mean(train_ssim_))
      train_psnr.append(Mean(psnr_))
      #val_loss.append(Mean(val_loss_))
      #val_ssim.append(Mean(val_ssim_))
      #val_psnr.append(Mean(val_psnr_))

    # Stop training if the model does not improve val loss 
    if early_stopping:
      break

    results['loss'] = train_loss
    results['ssim'] = train_ssim
    results['psnr'] = train_psnr
    results['val_loss'] = val_loss
    results['val_ssim'] = val_ssim
    results['val_psnr'] = val_psnr

  return results