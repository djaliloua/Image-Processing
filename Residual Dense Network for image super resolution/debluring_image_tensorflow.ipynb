{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Copy of Final_Project_Deblurring_REDS.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN1yw4y5KRXY"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQBMccp_KRXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7672d3-f4f5-49ae-bb22-eecaa208af59"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nx888tsGKRXz"
      },
      "source": [
        "import os\n",
        "import time,io,shutil,PIL,datetime\n",
        "from PIL import Image\n",
        "#import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "\n",
        "#import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers, models\n",
        "from IPython import display\n",
        "#tf.config.set_soft_device_placement(True)\n",
        "#tf.debugging.set_log_device_placement(True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSI0k6tUKRX5"
      },
      "source": [
        "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.98)\n",
        "sess = tf.compat.v1.Session(config = tf.compat.v1.ConfigProto(gpu_options=gpu_options))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpF7m_XbKRX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a075a41f-b3de-4fae-ef7f-fa68cebc67b2"
      },
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkJSRonKRX_"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(0.0001,.9)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5vtj3kuKRYD"
      },
      "source": [
        "# Chnage the working directory on which i have stored the images\n",
        "# os.chdir(\"D:\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLvTjxxdKRYJ"
      },
      "source": [
        "def load(image_file):\n",
        "    image = tf.io.read_file(image_file)\n",
        "    image = tf.image.decode_png(image)\n",
        "\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    return image"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppl3d0D9KRYP"
      },
      "source": [
        "def resize(input_image, height=286, width=286):\n",
        "    input_image = tf.image.resize(input_image, [height, width],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    return input_image"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cwLVgKVKRYX"
      },
      "source": [
        "def random_crop1(input_image):\n",
        "    stacked_image = tf.stack(input_image, axis=0)\n",
        "    cropped_image = tf.image.random_crop(\n",
        "      stacked_image, size=[ 256, 256, 3],seed=30)\n",
        "\n",
        "    return cropped_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2hkEnSKRYZ"
      },
      "source": [
        "# normalizing the images to [0, 1]\n",
        "\n",
        "def normalize(input_image):\n",
        "    input_image = (input_image / 255)\n",
        "    return input_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "DkLu8BRwKRYb"
      },
      "source": [
        "#@tf.function\n",
        "def random_jitter1(input_image):\n",
        "    # resizing to 286 x 286 x 3\n",
        "    input_image = resize(input_image, 512,512)\n",
        "    input_image = random_crop1(input_image)\n",
        "    \n",
        "    return input_image\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78X1nXBhKRYd"
      },
      "source": [
        "def load_train(path):\n",
        "    im = load(path)\n",
        "    im = random_jitter1(im)\n",
        "    im = normalize(im)\n",
        "    return im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2WAO4b6KRYf"
      },
      "source": [
        "                             Define Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUZJ3K-iKRYg"
      },
      "source": [
        "batch_size = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgoHwRFSKRYi"
      },
      "source": [
        "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIgSX5rmKRYj"
      },
      "source": [
        "train_blur0 = tf.data.Dataset.list_files(\"D:/train_blur/*/*.png\",seed=4)\n",
        "train_blur = train_blur0.map(load_train,num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsNyocD5KRYk"
      },
      "source": [
        "train_sharp = tf.data.Dataset.list_files(\"D:/train_sharp/*/*.png\",seed=4)\n",
        "train_sharp = train_sharp.map(load_train,num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCvUKZ2RKRYm"
      },
      "source": [
        "train_dataset = tf.data.Dataset.zip((train_blur,train_sharp)).shuffle(500).batch(batch_size).repeat()\n",
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbPSVNoXKRYo"
      },
      "source": [
        "val_blur0 = tf.data.Dataset.list_files(\"D:/val_blur/*/*.png\",seed=5)\n",
        "val_blur = val_blur0.map(load_train,num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCMhE1IdKRYp"
      },
      "source": [
        "val_sharp = tf.data.Dataset.list_files(\"D:/val_sharp/*/*.png\",seed=5)\n",
        "val_sharp = val_sharp.map(load_train,num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgb0gcaJKRYr"
      },
      "source": [
        "val_dataset = tf.data.Dataset.zip((val_blur,val_sharp)).batch(batch_size)\n",
        "val_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY0z0LQXKRYt"
      },
      "source": [
        "test_blur0 = tf.data.Dataset.list_files(\"D:/test_blur/*/*.png\",seed=41)\n",
        "test_blur = test_blur0.map(load_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjBF3TpwKRYv"
      },
      "source": [
        "test_sharp = tf.data.Dataset.list_files(\"D:/test_sharp/*/*.png\",seed=41)\n",
        "test_sharp = test_sharp.map(load_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NER3g5DWKRYv"
      },
      "source": [
        "test_dataset = tf.data.Dataset.zip((test_blur,test_sharp)).batch(batch_size)\n",
        "test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1SVxvDHdKRYw"
      },
      "source": [
        "# plot a sample of blurred image\n",
        "for t in train_blur.take(1):\n",
        "    plt.imshow(t[:,:,0],cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    #plt.title(\"shape {}\".format(t.shape))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GOZ_PsaKRYy"
      },
      "source": [
        "# Plot a sample of sharp image\n",
        "for t in train_sharp.take(1):\n",
        "    plt.imshow(t[:,:,0],cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    #plt.title(\"shape {}\".format(t.shape))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8_3vhZ8KRYz"
      },
      "source": [
        "def block(x1,f,size,name = None):\n",
        "    \n",
        "    inp = x1\n",
        "    \n",
        "    x = layers.Conv2D(f,size,strides=1,padding=\"same\")(x1)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x2 = layers.Concatenate()([x,inp])\n",
        "    \n",
        "    \n",
        "    \n",
        "    x3 = layers.Conv2D(f,size,strides=1,padding=\"same\")(x2)\n",
        "    x3 = layers.LeakyReLU(0.2)(x3)\n",
        "    x3 = layers.Concatenate()([x3,x2,inp])\n",
        "    #\n",
        "    \n",
        "    \n",
        "    x4 = layers.Conv2D(f,size,strides=1,padding=\"same\")(x3)\n",
        "    x4 = layers.LeakyReLU(0.2)(x4)\n",
        "    x4 = layers.Concatenate()([x4,x3,x2,inp])\n",
        "    #\n",
        "    \n",
        "    x5 = layers.Conv2D(f,size,strides=1,padding=\"same\")(x4)\n",
        "    x5 = layers.LeakyReLU(0.2)(x5)\n",
        "    x5 = layers.Concatenate()([x5,x4,x3,x2,inp])\n",
        "    \n",
        "    # final block\n",
        "    x7 = layers.Conv2D(f,1,strides=1,padding=\"same\",name=\"block{}_conv\".format(name))(x5)\n",
        "    # residlual learning RL\n",
        "    \n",
        "    RL = layers.Add()([x7,inp])\n",
        "    \n",
        "    return RL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubMrZMvvKRY1"
      },
      "source": [
        "def block_s(x1,f,size,name = None):\n",
        "    \n",
        "    inp = x1\n",
        "    \n",
        "    x = layers.Conv2D(f,(size,size),strides=1,padding=\"same\")(x1)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x2 = layers.Concatenate()([x,inp])\n",
        "    \n",
        "    \n",
        "    x3 = layers.Conv2D(f,1,padding=\"same\")(x2)\n",
        "    x3 = layers.Conv2D(f,(size,size),strides=1,padding=\"same\")(x3)\n",
        "    x3 = layers.LeakyReLU(0.2)(x3)\n",
        "    x3 = layers.Concatenate()([x3,x2,inp])\n",
        "    #\n",
        "    x4 = layers.Conv2D(f,(1,1),strides=1,padding=\"same\")(x3)\n",
        "    \n",
        "    x4 = layers.Conv2D(f,(size,size),strides=1,padding=\"same\")(x4)\n",
        "    x4 = layers.LeakyReLU(0.2)(x4)\n",
        "    x4 = layers.Concatenate()([x4,x3,x2,inp])\n",
        "    #\n",
        "    x5 = layers.Conv2D(f,(1,1),strides=1,padding=\"same\")(x4)\n",
        "    \n",
        "    x5 = layers.Conv2D(f,(size,size),strides=1,padding=\"same\")(x5)\n",
        "    x5 = layers.LeakyReLU(0.2)(x5)\n",
        "    x5 = layers.Concatenate()([x5,x4,x3,x2,inp])\n",
        "    \n",
        "    # final block\n",
        "    x7 = layers.Conv2D(f,(1,1),strides=1,padding=\"same\")(x5)\n",
        "    # residlual learning RL\n",
        "    \n",
        "    RL = layers.Add()([x7,inp])\n",
        "    \n",
        "    return RL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8OpsJU4KRY3"
      },
      "source": [
        "def block_c(x1,f,size,name = None):\n",
        "    \n",
        "    inp0=x1\n",
        "    x1 = layers.MaxPooling2D()(x1)\n",
        "    inp = x1\n",
        "    x = layers.Conv2D(f,size,strides=1,padding=\"same\",use_bias = False)(x1)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x2 = layers.Concatenate()([x,inp,x1])\n",
        "    \n",
        "    \n",
        "    x33 = layers.Conv2D(f,1,padding=\"same\",use_bias = False)(x2)\n",
        "    x3 = layers.Conv2D(f,size,strides=1,padding=\"same\",use_bias = False)(x33)\n",
        "    x3 = layers.LeakyReLU(0.2)(x3)\n",
        "    x3 = layers.Concatenate()([x33,x3,x2,inp])\n",
        "    #\n",
        "    x44 = layers.Conv2D(f,1,padding=\"same\",use_bias = False)(x3)\n",
        "    x4 = layers.Conv2D(f,size,strides=1,padding=\"same\",use_bias = False)(x44)\n",
        "    x4 = layers.LeakyReLU(0.2)(x4)\n",
        "    x4 = layers.Concatenate()([x44,x4,x3,x2,inp])\n",
        "    #\n",
        "    x55 = layers.Conv2D(f,1,padding=\"same\",use_bias = False)(x4)\n",
        "    x5 = layers.Conv2D(f,size,strides=1,padding=\"same\",use_bias = False)(x55)\n",
        "    x5 = layers.LeakyReLU(0.2)(x5)\n",
        "    x5 = layers.Concatenate()([x55,x5,x4,x3,x2,inp])\n",
        "    \n",
        "    # final block\n",
        "    x5 = tf.nn.depth_to_space(x5,2)\n",
        "    x7 = layers.Conv2D(f,1,strides=1,padding=\"same\",name=\"block{}_conv\".format(name))(x5)\n",
        "    # residlual learning RL\n",
        "    \n",
        "    RL = layers.Add()([x7,inp0])\n",
        "    \n",
        "    return RL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGDil6SoKRY5"
      },
      "source": [
        "def block_g(x1,f,size,name = None):\n",
        "    \n",
        "    inp = x1\n",
        "    \n",
        "    x = layers.Conv2D(f,size,strides=1,padding=\"same\")(x1)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x2 = layers.Concatenate()([x,inp])\n",
        "    \n",
        "    \n",
        "    x3 = tf.nn.depth_to_space(x2,2)\n",
        "    x3 = layers.Conv2D(f,size,strides=2,padding=\"same\",use_bias = False)(x3)\n",
        "    x3 = layers.LeakyReLU(0.2)(x3)\n",
        "    x3 = layers.Concatenate()([x3,x2,inp])\n",
        "    #\n",
        "    x4 = tf.nn.depth_to_space(x3,2)\n",
        "    x4 = layers.Conv2D(f,size,strides=2,padding=\"same\",use_bias = False)(x4)\n",
        "    x4 = layers.LeakyReLU(0.2)(x4)\n",
        "    x4 = layers.Concatenate()([x4,x3,x2,inp])\n",
        "    #\n",
        "    x5 = tf.nn.depth_to_space(x4,2)\n",
        "    x5 = layers.Conv2D(f,size,strides=2,padding=\"same\",use_bias = False)(x5)\n",
        "    x5 = layers.LeakyReLU(0.2)(x5)\n",
        "    x5 = layers.Concatenate()([x5,x4,x3,x2,inp])\n",
        "    \n",
        "    # final block\n",
        "    x7 = layers.Conv2D(f,1,strides=1,padding=\"same\",name=\"block{}_conv\".format(name))(x5)\n",
        "    # residlual learning RL\n",
        "    \n",
        "    RL = layers.Add()([x7,inp])\n",
        "    \n",
        "    return RL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuNWMffXKRY7"
      },
      "source": [
        "#from tensorflow.keras.layers import Lambda\n",
        "\n",
        "def Residual_Net(input_shape=(256,256,3)):\n",
        "    \n",
        "    input0 = layers.Input(shape=input_shape)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # shallow features extraction\n",
        "    sf1 = layers.Conv2D(64,7,strides=2,padding=\"same\",use_bias = False)(input0) \n",
        "    \n",
        "    sf2 = layers.Conv2D(64,3,strides=1,padding=\"same\",use_bias = False)(sf1) \n",
        "    # Blocks\n",
        "    B1 = block_s(sf2,64,3,1)\n",
        "    B2 = block_s(B1,64,3,2)\n",
        "    B3 = block_s(B2,64,3,3)\n",
        "    B4 = block_c(B3,64,3,4)\n",
        "    #B5 = block(B4,64,3)\n",
        "    \n",
        "    # Global features fusion\n",
        "    \n",
        "    GFF = layers.Concatenate()([B1,B2,B3,B4])\n",
        "    GFF = layers.Conv2D(64,1,strides=1,padding=\"same\",use_bias = False)(GFF) \n",
        "    GFF = layers.Conv2D(64,3,strides=1,padding=\"same\",use_bias = False)(GFF) # \n",
        "    \n",
        "    # Global Residual Learning GRL\n",
        "    GRL = layers.Add()([GFF,sf1]) # \n",
        "    \n",
        "    # ESPCNN\n",
        "    \n",
        "    x = layers.Conv2D(64,5,activation=\"relu\",padding=\"same\")(GRL)\n",
        "    x = layers.Conv2D(64,3,activation=\"relu\",padding=\"same\",use_bias = False)(x)\n",
        "    x = layers.Conv2D(64,3,activation=\"relu\",padding=\"same\",use_bias = False)(x)\n",
        "    x = tf.nn.depth_to_space(x,4)\n",
        "    \n",
        "    \n",
        "    \n",
        "    x = layers.Conv2D(3,7,padding=\"same\",strides=2,use_bias = False)(x)\n",
        "    \n",
        "    #\n",
        "    \n",
        "    \n",
        "    x = layers.Add()([x,input0])\n",
        "    x = layers.Activation(\"tanh\")(x)\n",
        "    \n",
        "    \n",
        "\n",
        "    model = models.Model(input0,x)\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJa7CQrAKRY9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SSeObEbnKRY-"
      },
      "source": [
        "generator = Residual_Net()\n",
        "#model = efficient1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_2JEPblKRZA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xrno5IpkKRZB"
      },
      "source": [
        "def generate_images(model, test_input, tar,tr=True):\n",
        "    prediction = model(test_input, training=tr)\n",
        "    fig = plt.figure(figsize=(15,15))\n",
        "\n",
        "    display_list = [test_input[0], tar[0], prediction[0]]\n",
        "    p = tf.convert_to_tensor(prediction)\n",
        "    ssim = tf.constant(tf.image.ssim(p,tar,1.0)).numpy()\n",
        "    psnr = tf.constant(tf.image.psnr(p,tar,1.0)).numpy()\n",
        "    #noisy_ssim = tf.constant(tf.image.ssim(test_input,tar,1.0)).numpy()\n",
        "    #noisy_psnr = tf.constant(tf.image.psnr(test_input,tar,1.0)).numpy()\n",
        "    title = ['Input Image ', \n",
        "             'Ground Truth', \n",
        "             'Predicted Image  '+str(\"PSNR/SSIM:{:1.4f}/{:3.4f}\".format(list(psnr)[0],list(ssim)[0]))]\n",
        "    for i in range(3):\n",
        "        plt.subplot(3, 3, i+1)\n",
        "        plt.title(title[i])\n",
        "        # getting the pixel values between [0, 1] to plot it.\n",
        "        plt.axis('off')\n",
        "        plt.imshow(display_list[i]*0.5+0.5)\n",
        "        plt.grid(True)\n",
        "        \n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0dVxx3hOKRZC"
      },
      "source": [
        "for test,tar in test_dataset.take(10):\n",
        "    generate_images(generator,test,tar,tr=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQrLz4AWKRZG"
      },
      "source": [
        "import functools\n",
        "\n",
        "psnr_metric = functools.partial(tf.image.psnr, max_val=1.)\n",
        "psnr_metric.__name__ = 'psnr'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzKD4cU_KRZH"
      },
      "source": [
        "ssim_metric = functools.partial(tf.image.ssim,max_val=1.)\n",
        "ssim_metric.__name__ = \"ssim\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPMjXixYKRZI"
      },
      "source": [
        "num_epochs = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I41PVXuYKRZJ"
      },
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "def vgg_layers(layer_names):\n",
        "    vgg = VGG19(include_top=False,weights=\"imagenet\",input_shape=(256,256,3))\n",
        "    vgg.trainable = False\n",
        "    out = [vgg.get_layer(name).output for name in layer_names]\n",
        "    model = models.Model([vgg.input],out)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxH-uPnGKRZK"
      },
      "source": [
        "layer_name = [\"block3_conv3\"]\n",
        "feature_extractor = vgg_layers(layer_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNPwoeITKRZL"
      },
      "source": [
        "def perceptual_loss(y_true,y_pred):\n",
        "    f_true = feature_extractor(y_true)\n",
        "    f_pred = feature_extractor(y_pred)\n",
        "    return 0.80*tf.reduce_mean(tf.abs(f_true-f_pred))+0.20*tf.reduce_mean(tf.abs(y_true-y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ptbi6N4KRZM"
      },
      "source": [
        "tf.config.experimental_run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBnjkqaqKRZN"
      },
      "source": [
        "generator.compile(loss= perceptual_loss,optimizer=optimizer,metrics=[psnr_metric,ssim_metric])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJhqO0LoKRZN"
      },
      "source": [
        "# set up checkpoint\n",
        "checkpoint_path = \"training_REDS\\cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "# create a callbacks for saving weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_dir,\n",
        "                                                save_weights_only=True,\n",
        "                                                verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X1YkCQsKRZO"
      },
      "source": [
        "shutil.rmtree(\"story\",ignore_errors=True)\n",
        "log_dir = os.path.join(\"story\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "#logdir = os.path.join(\"log\",\"scalar\")\n",
        "callbacks  = [# Callback to interrupt the training if the validation loss/metrics stops improving for some epochs:\n",
        "    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss',\n",
        "                                     restore_best_weights=True),tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1),cp_callback]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tk3E6PfQKRZP"
      },
      "source": [
        "start = time.time()\n",
        "history = generator.fit(\n",
        "    train_dataset, steps_per_epoch=21000, epochs=num_epochs,\n",
        "    validation_data=val_dataset,validation_steps=3000,\n",
        "    verbose=1,callbacks=callbacks)\n",
        "print(f\"Total number of Days taken for running the model: {(time.time()-start)/(3600*24)} Days\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ab-F7naKRZQ"
      },
      "source": [
        "fig, ax = plt.subplots(3, 2, figsize=(10, 5), sharex='col')\n",
        "ax[0, 0].set_title(\"loss\")\n",
        "ax[0, 1].set_title(\"val-loss\")\n",
        "ax[1, 0].set_title(\"psnr\")\n",
        "ax[1, 1].set_title(\"val-psnr\")\n",
        "ax[2, 0].set_title(\"ssim\")\n",
        "ax[2, 1].set_title(\"val-ssim\")\n",
        "\n",
        "\n",
        "ax[0, 0].plot(history.history['loss'])\n",
        "ax[0, 1].plot(history.history['val_loss'])\n",
        "ax[1, 0].plot(history.history['psnr'])\n",
        "ax[1, 1].plot(history.history['val_psnr'])\n",
        "ax[2, 0].plot(history.history['ssim'])\n",
        "ax[2, 1].plot(history.history['val_ssim'])\n",
        "\n",
        "fig.savefig(\"motion_blur.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Iqegp5QKRZS"
      },
      "source": [
        "for noisy,target in test_dataset.take(10):\n",
        "    generate_images(generator,noisy,target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alv5VWvDKRZT"
      },
      "source": [
        "f = []\n",
        "for n,t in test_dataset.take(200):\n",
        "    f.append(generate_images(generator,n,t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnICO6VqKRZW"
      },
      "source": [
        "def fig2img(fig):\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf)\n",
        "    plt.close()\n",
        "    buf.seek(0)\n",
        "    img = Image.open(buf)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9VXyfaXKRZX"
      },
      "source": [
        "frame = []\n",
        "for i in f:\n",
        "    frame.append(fig2img(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4Ltq93NKRZY"
      },
      "source": [
        "frame[0].save(\"test0.png\")\n",
        "frame[1].save(\"test1.png\")\n",
        "frame[2].save(\"test2.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TuajnrPKRZZ"
      },
      "source": [
        "frame[0].save(\"Result_REDS.gif\",format=\"GIF\",append_images=frame[1:],save_all=True,duration=2000,loop=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKD5lYN1KRZZ"
      },
      "source": [
        "ls {checkpoint_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvozhW-KKRZb"
      },
      "source": [
        "generator.load_weights(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB0eIkbqKRZc"
      },
      "source": [
        "m = generator.evaluate(test_dataset.take(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sudb9gi6KRZd"
      },
      "source": [
        "m"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}